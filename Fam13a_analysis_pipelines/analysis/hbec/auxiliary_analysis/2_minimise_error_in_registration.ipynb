{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook calculates the two types of errors described in [notebook: 1_errors_in_registration](1_errors_in_registration.ipynb) for a range of registration parameters, with the aim of choosing a set of parameters to minimise both errors. The errors are calculated for a specific experiment and specific NT and gAA well. These types of wells are chosen because they should represent the wells with the highest and lowest amount of motion, respectively. The `report/registration-parameter-tuning.pptx` file contains the plots from this notebook for all experiments. The `pptx` file also contains the chosen registration parameters for each experiment.\n",
    "\n",
    "- As described in the [previous notebook](1_errors_in_registration.ipynb), there are two types of errors which are plotted here:\n",
    "\n",
    "    1. root mean square error (RMSE): registration process error\n",
    "    2. similarity error: forwards and backwards pass difference\n",
    "    \n",
    "    \n",
    "- The size of these errors is determined by the values and combination of the 3 tuneable registration parameters: `REGION_SIZE`, `PTRN_SIZE`, `MAX_WINDOW`. The impact of these parameters being too small or too large is summarised in the table below:\n",
    "\n",
    "| Registration Parameter  | Too small                                                                                                 | Too large                                                                                         |\n",
    "|-------------------------|-----------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\n",
    "| PTRN_SIZE               | Increases spurious matches between pattern in region at timestep, T and pattern in region at timestep T+1 | The content of the pattern has changed between frames and therefore a match cannot be found.      |\n",
    "| REGION_SIZE             | The pattern could move outside the region and therefore a match cannot be found.                          | The matching patterns between two time steps could be wrong, leading to the wrong velocity vector |\n",
    "| MAX_WINDOW              | Long processing time, weaker signals due to lower intensity.                                              | Smearing out the signal beyond recognition, leading to poor matching and wrong velocity vectors   |\n",
    "\n",
    "- A single example of  the two errors plotted against `PTRN_SIZE` is shown in the figure below. Here the RMSE clearly increases with increasing `PTRN_SIZE` and the similarity error decreases with increasing `PTRN_SIZE`. In this situation a compromise must be made between the errors and so a `PTRN_SIZE` of ~9 can be chosen\n",
    "\n",
    "<img src=\"../markdown_images_for_notebooks/example-errors-graphs-compromise.PNG\">\n",
    "\n",
    "- Since three parameters must be chosen, this notebook plots the errors against the registration parameters in two different ways as described below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from fam13a import utils, image, register\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import altair as alt\n",
    "import time\n",
    "from altair_saver import save\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_ROOT = utils.here()\n",
    "# declare the data input directory\n",
    "INTERIM_HBEC_ROOT = os.path.join(PROJ_ROOT, 'data', 'interim', 'hbec')\n",
    "print(os.listdir(INTERIM_HBEC_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose experiment data to load\n",
    "EXP_ID = 'N67030-59_PBS'\n",
    "\n",
    "INTERIM_ROOT = os.path.join(PROJ_ROOT, INTERIM_HBEC_ROOT, EXP_ID)\n",
    "\n",
    "# declare the various output directories\n",
    "PROCESSED_ROOT = os.path.join(PROJ_ROOT, 'data', 'processed', 'hbec', EXP_ID)\n",
    "ROI_ROOT = os.path.join(PROCESSED_ROOT, 'roi')\n",
    "SEG_ROOT = os.path.join(PROCESSED_ROOT, 'segmented', 'movement')\n",
    "MAX_FRAME_ROOT = os.path.join(PROCESSED_ROOT, 'max_frame')\n",
    "\n",
    "OUTPUT_ROOT = os.path.join(PROCESSED_ROOT, 'register')\n",
    "# set level of parallelisation\n",
    "NCPUS = 16\n",
    "# define the common file extension used in the input data files\n",
    "EXTENSION = '.ome.tif'\n",
    "\n",
    "# find all relevant data files in the data directory \n",
    "files = sorted([_f for _f in os.listdir(INTERIM_ROOT) if _f.endswith('tif')])\n",
    "# remove the extension the file names, se we keep only the bit with useful information\n",
    "files = [_f.split(EXTENSION)[0] for _f in files]\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'NT_PBS_3_1_MMStack_Pos0'\n",
    "    \n",
    "# load each of the associated interim/processed files\n",
    "frames = utils.frames_from_stack(os.path.join(INTERIM_ROOT, f'{file}{EXTENSION}'))\n",
    "roi = np.load(os.path.join(ROI_ROOT, f'{file}.npy'))\n",
    "max_frame = np.load(os.path.join(MAX_FRAME_ROOT, f'{file}.npy'))\n",
    "movement_mask = np.load(os.path.join(SEG_ROOT, f'{file}.npy')).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive time and negative time axis frames:\n",
    "frames_positive = frames.copy()\n",
    "frames_negative = frames[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a very simple wrapper around the register.estimate_shifts functon \n",
    "# purely to allow the inclusion of a progress bar during processing\n",
    "def process(regions_pair):\n",
    "    return register.estimate_shifts(*regions_pair, ptrn_size=ptrn_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the reg process, generate the velocity fields, generate the errors and return a dict containing\n",
    "# the velocity fields, downsampled mask and mean, std rmse too\n",
    "def run_registration_process_with_minimised_errors(zoomed_frames, zoomed_movement_mask, mw, ps, rs):\n",
    "    reg_process = register.run_registration_process_for_auxillary_analysis(\n",
    "        zoomed_frames, zoomed_movement_mask, mw, ps, rs, NCPUS\n",
    "    )\n",
    "    # isolate the errors\n",
    "    errors = reg_process['errors']\n",
    "    # get min of errors (RMSE) per timestep and then average over time too\n",
    "    min_errors = errors.min(axis=3);\n",
    "    min_errors_mean = min_errors.mean(axis=0)\n",
    "    # remove the errors key in the dictionary and add the mean and std rmse\n",
    "    reg_process.pop('errors')\n",
    "    reg_process['mean_rmse'] = min_errors_mean.mean()\n",
    "    reg_process['std_rmse'] = min_errors_mean.std()\n",
    "    \n",
    "    return reg_process\n",
    "\n",
    "# generating an average and a std for similarirty across the chosen sample\n",
    "def calculate_similarity(\n",
    "    zoomed_frames_positive, zoomed_frames_negative, zoomed_movement_mask, mw, ps, rs\n",
    "):\n",
    "    \n",
    "    positive = run_registration_process_with_minimised_errors(\n",
    "        zoomed_frames_positive, zoomed_movement_mask, mw, ps, rs\n",
    "    )\n",
    "    negative = run_registration_process_with_minimised_errors(\n",
    "        zoomed_frames_negative, zoomed_movement_mask, mw, ps, rs\n",
    "    )\n",
    "    \n",
    "    difference = {}\n",
    "    difference['mags'] = np.abs(positive['mags'] - negative['mags'])\n",
    "    # fractinal difference in speed (mags)\n",
    "    difference['mags'] /= np.where(positive['mags'] == 0, np.inf, positive['mags'])\n",
    "\n",
    "    # too look at differences between angles, we must take the cosine between the \n",
    "    # positive and (-)negative vectors\n",
    "    difference['cosines'] = (\n",
    "        (positive['vx']*(-1)*negative['vx']) + (positive['vy']*(-1)*negative['vy']) \n",
    "    )\n",
    "    # get rid of numerical errors (|cosine| > 1)\n",
    "    difference['cosines'][difference['cosines']>1] = 1\n",
    "    difference['cosines'][difference['cosines']<-1] = -1\n",
    "    difference['angles'] = (180/math.pi) * (np.arccos(difference['cosines']))\n",
    "    \n",
    "    sub_mask = positive['sub_mask']\n",
    "\n",
    "    result_mean = {}\n",
    "    result_std = {}\n",
    "    for key in ['mags','angles']:\n",
    "        result_mean[key] = np.mean(difference[key][sub_mask])\n",
    "        result_std[key] = np.std(difference[key][sub_mask])\n",
    "    result_mean['positive_rmse'] = positive['mean_rmse']\n",
    "    result_std['positive_rmse'] = positive['std_rmse']\n",
    "    \n",
    "    max_mag_position = np.unravel_index(positive['mags'].argmax(), positive['mags'].shape)\n",
    "        \n",
    "    return result_mean, result_std, max_mag_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose small patch to focus on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axes\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(max_frame);\n",
    "ax.imshow(movement_mask, alpha=0.1)\n",
    "# Create a Rectangle patch\n",
    "top_left_corner = (500,1000)\n",
    "box_width = 200\n",
    "box_height = 200\n",
    "rect = patches.Rectangle(top_left_corner,box_width,box_height,linewidth=1,edgecolor='r',facecolor='none')\n",
    "\n",
    "# Add the patch to the Axes\n",
    "ax.add_patch(rect)\n",
    "plt.title(f'{EXP_ID}-{file}-sample-patch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get limits of the bounding box\n",
    "box_x_lim = (top_left_corner[1], top_left_corner[1] + box_height)\n",
    "box_y_lim = (top_left_corner[0], top_left_corner[0] + box_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop through various registration parameters\n",
    "- with NCPUS=32, the following loop takes ~600s for a 200x200 patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# window size to take along the time dimension\n",
    "max_window_list = ['1', '1_5', '5', '7']\n",
    "# set the sizes of the pattern and search regions\n",
    "ptrn_size_list = [5,7,9,11,13]\n",
    "region_size_list = [9,11,13,15,17,19,21]\n",
    "\n",
    "# results dataframe\n",
    "df = pd.DataFrame(columns = ['max_window', 'ptrn_size', 'region_size', 'mean', 'std'])\n",
    "start_time = time.time()\n",
    "for max_window_ in max_window_list:\n",
    "     \n",
    "    if '_' in max_window_:\n",
    "        # take every nth frame (alternative to max over time-widow since we set MAX_WINDOW = 1)\n",
    "        nth = int(max_window_.split('_')[1])\n",
    "        zoomed_frames_positive = frames_positive[::nth,slice(*(box_x_lim)),slice(*(box_y_lim))]\n",
    "        zoomed_frames_negative = frames_negative[::nth,slice(*(box_x_lim)),slice(*(box_y_lim))]\n",
    "        max_window = int(max_window_.split('_')[0])\n",
    "    else:\n",
    "        zoomed_frames_positive = frames_positive[:,slice(*(box_x_lim)),slice(*(box_y_lim))]\n",
    "        zoomed_frames_negative = frames_negative[:,slice(*(box_x_lim)),slice(*(box_y_lim))]\n",
    "        max_window = int(max_window_)\n",
    "    zoomed_motion_mask = movement_mask[slice(*(box_x_lim)),slice(*(box_y_lim))]\n",
    "        \n",
    "    for ptrn_size_ in ptrn_size_list:\n",
    "        ptrn_size = (ptrn_size_,)*2\n",
    "        \n",
    "        for region_size_ in region_size_list:\n",
    "            region_size = (region_size_,)*2\n",
    "            # make sure the pattern fits inside the region and when scanning in the region, it must have more than 9 options (so min_options=16)\n",
    "            if region_size_ - ptrn_size_ > 3:\n",
    "                mean, std, max_mag_position = calculate_similarity(\n",
    "                    zoomed_frames_positive, zoomed_frames_negative, zoomed_motion_mask,\n",
    "                    max_window, ptrn_size, region_size\n",
    "                )\n",
    "                for key in ['mags','angles']:\n",
    "                    df = df.append({\n",
    "                        'max_window': max_window_, \n",
    "                        'ptrn_size': ptrn_size_, \n",
    "                        'region_size': region_size_, \n",
    "                        'mean': mean[key], \n",
    "                        'std': std[key],\n",
    "                        'mean_pos_rmse': mean['positive_rmse'],\n",
    "                        'std_pos_rmse': std['positive_rmse'],\n",
    "                        'field': key,\n",
    "                        'max_mag_position': max_mag_position\n",
    "                    }, ignore_index=True)\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot similarity, rmse vs registration parameters\n",
    "\n",
    "- 4 plots are produced and saved in this section:\n",
    "    1. rmse-view-1.svg\n",
    "    2. sim-view-1.svg\n",
    "    3. rmse-view-2.svg\n",
    "    4. sim-view-2.svg\n",
    "- \"view-1\" plots the error against the `PTRN_SIZE` on the x-axis, with the different colours of the points representing various `REGION_SIZE` values. The variation in error with respect to `MAX_WINDOW` is captured on different rows of the view. \n",
    "- \"view-2\" plots the error against the `REGION_SIZE` on the x-axis, with the different colours of the points representing various `MAX_WINDOW` values. The variation in error with respect to `PTRN_SIZE` is captured on different rows of the view. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff=df.copy()\n",
    "df.drop(columns=['max_mag_position'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate base chart object with all the needed data\n",
    "base = alt.Chart(df).transform_calculate(\n",
    "    ymin='datum.mean_pos_rmse-datum.std_pos_rmse',\n",
    "    ymax='datum.mean_pos_rmse+datum.std_pos_rmse'\n",
    ")\n",
    "# construct the points and error bars plots from the base chart\n",
    "points = base.mark_point(size=100).encode(\n",
    "    y=alt.X('mean_pos_rmse:Q', axis=alt.Axis(title='rmse mean'),\n",
    "        scale=alt.Scale(zero=False)),\n",
    "    x=alt.X('ptrn_size:N', axis=alt.Axis(title='ptrn size')),\n",
    "    color='region_size:N'\n",
    ")\n",
    "errors = base.mark_errorbar().encode(\n",
    "    x=alt.X('ptrn_size:N', axis=alt.Axis(title='ptrn size')),\n",
    "    y=alt.Y('ymin:Q', axis=alt.Axis(title='mean_pos_rmse')),\n",
    "    y2='ymax:Q',\n",
    "    color='region_size:N'\n",
    ")\n",
    "\n",
    "k = alt.layer(points).facet(\n",
    "    row=alt.Column('max_window:N',),\n",
    "    column=alt.Row('field:N',),\n",
    ").configure_axis(\n",
    "    labelFontSize=16,\n",
    "    titleFontSize=20\n",
    ").configure_legend(\n",
    "    labelFontSize = 20,\n",
    "    titleFontSize = 20\n",
    ").configure_header(\n",
    "    labelFontSize=20,\n",
    "    titleFontSize = 24\n",
    ").resolve_scale(\n",
    "    y='independent'\n",
    ")\n",
    "save(k, f'{EXP_ID}-{file}-rmse-view-1.svg'); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate base chart object with all the needed data\n",
    "base = alt.Chart(df).transform_calculate(\n",
    "    ymin='datum.mean-datum.std',\n",
    "    ymax='datum.mean+datum.std'\n",
    ")\n",
    "# construct the points and error bars plots from the base chart\n",
    "points = base.mark_point(size=100).encode(\n",
    "    y=alt.X('mean:Q', axis=alt.Axis(title='similarity mean')),\n",
    "    x=alt.X('ptrn_size:N', axis=alt.Axis(title='ptrn size')),\n",
    "    color='region_size:N'\n",
    ")\n",
    "errors = base.mark_errorbar().encode(\n",
    "    x=alt.X('ptrn_size:N', axis=alt.Axis(title='ptrn size')),\n",
    "    y=alt.Y('ymin:Q', axis=alt.Axis(title='mean')),\n",
    "    y2='ymax:Q',\n",
    "    color='region_size:N'\n",
    ")\n",
    "k = alt.layer(points).facet(\n",
    "    row=alt.Column('max_window:N',),\n",
    "    column=alt.Row('field:N',),\n",
    ").configure_axis(\n",
    "    labelFontSize=16,\n",
    "    titleFontSize=20\n",
    ").configure_legend(\n",
    "    labelFontSize = 20,\n",
    "    titleFontSize = 20\n",
    ").configure_header(\n",
    "    labelFontSize=20,\n",
    "    titleFontSize = 24\n",
    ").resolve_scale(\n",
    "    y='independent'\n",
    ")\n",
    "save(k, f'{EXP_ID}-{file}-sim-view-1.svg'); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate base chart object with all the needed data\n",
    "base = alt.Chart(df).transform_calculate(\n",
    "    ymin='datum.mean-datum.std',\n",
    "    ymax='datum.mean+datum.std'\n",
    ")\n",
    "# construct the points and error bars plots from the base chart\n",
    "points = base.mark_point(size=100).encode(\n",
    "    y=alt.Y('mean:Q', axis=alt.Axis(title='similarity mean')),\n",
    "    x=alt.X('region_size:N', axis=alt.Axis(title='region size')),\n",
    "    color='max_window:N'\n",
    ")\n",
    "errors = base.mark_errorbar().encode(\n",
    "    x=alt.X('region_size:N', axis=alt.Axis(title='region size')),\n",
    "    y=alt.Y('ymin:Q', axis=alt.Axis(title='mean')),\n",
    "    y2='ymax:Q',\n",
    "    color='max_window:N'\n",
    ")\n",
    "k=alt.layer(points).facet(\n",
    "    row=alt.Column('field:N',),\n",
    "    column=alt.Row('ptrn_size:N',),\n",
    ").configure_axis(\n",
    "    labelFontSize=16,\n",
    "    titleFontSize=20\n",
    ").configure_legend(\n",
    "    labelFontSize = 20,\n",
    "    titleFontSize = 20\n",
    ").configure_header(\n",
    "    labelFontSize=20,\n",
    "    titleFontSize = 24\n",
    ").resolve_scale(\n",
    "    y='independent'\n",
    ")\n",
    "save(k, f'{EXP_ID}-{file}-sim-view-2.svg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate base chart object with all the needed data\n",
    "base = alt.Chart(df).transform_calculate(\n",
    "    ymin='datum.mean_pos_rmse-datum.std_pos_rmse',\n",
    "    ymax='datum.mean_pos_rmse+datum.std_pos_rmse'\n",
    ")\n",
    "# construct the points and error bars plots from the base chart\n",
    "points = base.mark_point(size=100).encode(\n",
    "    y=alt.Y('mean_pos_rmse:Q', axis=alt.Axis(title='rmse mean'),\n",
    "        scale=alt.Scale(zero=False)),\n",
    "    x=alt.X('region_size:N', axis=alt.Axis(title='region size')),\n",
    "    color='max_window:N'\n",
    ")\n",
    "errors = base.mark_errorbar().encode(\n",
    "    x=alt.X('region_size:N', axis=alt.Axis(title='region size')),\n",
    "    y=alt.Y('ymin:Q', axis=alt.Axis(title='mean')),\n",
    "    y2='ymax:Q',\n",
    "    color='max_window:N'\n",
    ")\n",
    "k=alt.layer(points).facet(\n",
    "    row=alt.Column('field:N',),\n",
    "    column=alt.Row('ptrn_size:N',),\n",
    ").configure_axis(\n",
    "    labelFontSize=16,\n",
    "    titleFontSize=20\n",
    ").configure_legend(\n",
    "    labelFontSize = 20,\n",
    "    titleFontSize = 20\n",
    ").configure_header(\n",
    "    labelFontSize=20,\n",
    "    titleFontSize = 24\n",
    ").resolve_scale(\n",
    "    y='independent'\n",
    ")\n",
    "save(k, f'{EXP_ID}-{file}-rmse-view-2.svg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# view rmse and similarity error charts side by side\n",
    "\n",
    "Two images are produced in this section:\n",
    "1. rmse and similarity error in view-1\n",
    "2. rmse and similarity error in view-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(image1, image2, view):\n",
    "    # read images\n",
    "    img_A = mpimg.imread(image1)\n",
    "    img_B = mpimg.imread(image2)\n",
    "\n",
    "    # display images\n",
    "    plt.figure(figsize=(30,30))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(img_A);\n",
    "    plt.axis('off')\n",
    "    plt.title(f'{EXP_ID}-{file}-rmse')\n",
    "\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(img_B);\n",
    "    plt.axis('off')\n",
    "    plt.title(f'{EXP_ID}-{file}-sim')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "# run these shell commands to convert svg's to png's to view them side by side using the function above\n",
    "for error_type in ['rmse', 'sim']:\n",
    "    for view_type in [1,2]:\n",
    "        subprocess.call([\n",
    "            'convert', \n",
    "            f'{EXP_ID}-{file}-{error_type}-view-{view_type}.svg',\n",
    "            f'{EXP_ID}-{file}-{error_type}-view-{view_type}.png'\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(f'{EXP_ID}-{file}-rmse-view-1.png', f'{EXP_ID}-{file}-sim-view-1.png', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(f'{EXP_ID}-{file}-rmse-view-2.png', f'{EXP_ID}-{file}-sim-view-2.png', 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
