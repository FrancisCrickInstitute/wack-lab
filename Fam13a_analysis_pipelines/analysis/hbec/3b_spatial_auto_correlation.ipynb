{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The aim of this notebook is to estimate the amount of coordinated motion within a well. This is done by calculating the spatial auto-correlation for each average velocity vector field of all the wells in all experiments.\n",
    "\n",
    "### Coordinated motion & Spatial auto-correlation\n",
    "\n",
    "- Within our average velocity vector fields, there will be regions where a cluster of neighbouring regions are moving in a similar direction (coordinated motion) and there will non-coordinated regions too. \n",
    "- For a 2D scalar field, the calculation of local spatial auto-correlation, produces a 2D mask that shows where the scalar field is clustered, randomly distributed or dispersed. This calculation is carried out using the `pyslal/esda` Python library. Specifically, the local Geary's C value is a measure of local spatial auto-correlation. See the [original paper](https://onlinelibrary.wiley.com/doi/full/10.1111/gean.12164#:~:text=The%20Local%20Geary%20ci%20is%20a%20univariate%20statistic.,4) for more detail or [this video](https://www.youtube.com/watch?v=EKIKEeAw0W8&ab_channel=GeoDaSoftware) for a quick walk through. Some detail can also be found in Slides 8 to 15 in `reports/summary_meeting_presentations/09_09_20_summary.pptx`. \n",
    "- However, the velocity vector field is not a scalar field. Therefore, it is decomposed into three scalar fields: `speed`, `vx`, `vy`, where `vx`, `vy` are measures of the motion in the `x` and `y` directions, respectively. Therefore, three Geary's c masks are produced given a single velocity vector field.\n",
    "    - Decomposing the velocity vector field into `speed` and `angle` scalar fields was also considered. However, the angle scalar field had the issue of a sharp boundary between pixels where the angles were close to 0 and 360 degrees. \n",
    "- There is a single tuneable parameter: `NEIGHBORHOOD_SIZE`. This is tuned in [4_sensitivity_analysis_patch_size_vs_coordination_proportion](auxiliary_analysis/4_sensitivity_analysis_patch_size_vs_coordination_proportion.ipynb). When calculating Geary's C, the local neighbourhood of a pixel must be defined. For example, in the figure below, the red neighbourhood has `NEIGHBORHOOD_SIZE=3` and the green neighbourhood has `NEIGHBORHOOD_SIZE=5`.\n",
    "\n",
    "    <img src=\"markdown_images_for_notebooks/neighbourhood-size.PNG\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "import json\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from fam13a import utils, register\n",
    "from fam13a import spatial_autocorrelation as sac\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import time\n",
    "import pysal as ps\n",
    "from esda.geary_local import Geary_Local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_ROOT = utils.here()\n",
    "# declare the data input directory\n",
    "HBEC_ROOT = os.path.join(PROJ_ROOT, 'data', 'processed', 'hbec')\n",
    "# print list of experiment IDs\n",
    "print(os.listdir(HBEC_ROOT))\n",
    "\n",
    "experiment_list = os.listdir(HBEC_ROOT)\n",
    "\n",
    "# set level of parallelisation\n",
    "# the number of NCPUS is the number of files that will be processed in parallel.\n",
    "# For example, if the current node that the notebook is being run on is 32 cores, \n",
    "# then setting NCPUS to 4 will mean 4 files are processed in paraller and that 8\n",
    "# cores are used by each file for processing. The more cores provided to a file, \n",
    "# the shorter the processing time. Make sure that the available cores on this node\n",
    "# is greater than NCPUS.\n",
    "NCPUS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'N67030-59_8_perc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve image size of data given a single experiment\n",
    "def get_example_image_size(experiment_str):\n",
    "    # path to the register directory for given experiment\n",
    "    data_dir_path = os.path.join(HBEC_ROOT, experiment_str, 'register')\n",
    "    # consider only those directories in \"register\" that are directories (the other files could be .dvc files)\n",
    "    register_dirs = [name for name in os.listdir(data_dir_path) if os.path.isdir(os.path.join(data_dir_path, name))]\n",
    "    # isolate the path to an example max_frame array\n",
    "    max_frame_file_path = os.path.join(data_dir_path, register_dirs[0], 'max_frame.npy')\n",
    "    max_frame = np.load(max_frame_file_path)\n",
    "    array_shape = max_frame.shape\n",
    "    \n",
    "    return array_shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when calculating the gearys c value, significance testing is also carried out by \n",
    "# permutating the neighborhood of a pixel\n",
    "PERMUTATIONS = 999\n",
    "\n",
    "# require the image size of the 2D matrices that for which Geary's C will be calculated\n",
    "IMAGE_SIZE = get_example_image_size(experiment)\n",
    "\n",
    "# creating a neighbor patch - to define neighbors of any element in an image\n",
    "NEIGHBORHOOD_SIZE = 9\n",
    "NEIGHBORHOOD_PATCH = sac.create_neighbor_patch(NEIGHBORHOOD_SIZE)\n",
    "\n",
    "# create the spatial dictionaries (neighbors and weights)\n",
    "SPATIAL_DICT = sac.create_spatial_dicts(IMAGE_SIZE, NEIGHBORHOOD_PATCH)\n",
    "\n",
    "GEARYS_WEIGHT_OBJECT = ps.lib.weights.W(neighbors=SPATIAL_DICT['neighbors'], weights=SPATIAL_DICT['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_geary(mat):\n",
    "    lg = Geary_Local(\n",
    "        GEARYS_WEIGHT_OBJECT,\n",
    "        permutations=PERMUTATIONS,\n",
    "        n_jobs=1, \n",
    "        keep_simulations=False, \n",
    "        seed=42\n",
    "    ).fit(mat)\n",
    "\n",
    "    return lg\n",
    "\n",
    "def get_folder_information(exp_id):\n",
    "    # declare the various output directories\n",
    "    processed_root = os.path.join(HBEC_ROOT, exp_id)\n",
    "    register_root = os.path.join(processed_root, 'register')\n",
    "    output_root = os.path.join(processed_root, 'spatial_auto_correlation', 'neighbor_size_{}_geary'.format(NEIGHBORHOOD_SIZE))\n",
    "\n",
    "    # find all relevant data files in the data directory \n",
    "    file_ids = sorted([_d for _d in os.listdir(register_root) if os.path.isdir(os.path.join(register_root, _d))])\n",
    "\n",
    "    return processed_root, register_root, output_root, file_ids\n",
    "\n",
    "def process_files(file_id_):\n",
    "    # choose data file to load\n",
    "    file_root = os.path.join(register_root, file_id_)\n",
    "\n",
    "    # load the downsampled movement mask and the shifts matrix\n",
    "    shifts = np.load(os.path.join(file_root, 'shifts.npy'))\n",
    "    sub_movement_mask = np.load(os.path.join(file_root, 'mask.npy'))\n",
    "\n",
    "    # calculate average velocity field from shifts array\n",
    "    velocity_fields = register.calculate_mean_velocity_field(shifts)\n",
    "    norm_shifts = velocity_fields['normalised_velocity']\n",
    "    mags = velocity_fields['speed']\n",
    "\n",
    "    # we need gearys c index and p-values for speed, vx, vy - total 6 matrix results per \n",
    "    #  experiment: \n",
    "    #  gearys_i_speed, gearys_i_vx, gearys_i_vy, \n",
    "    #  gearys_p_speed, gearys_p_vx, gearys_p_vy\n",
    "    geary_list = [mags, norm_shifts[:,:,1], norm_shifts[:,:,0]] \n",
    "    geary_objects = []\n",
    "    for mat in tqdm(geary_list):\n",
    "        geary_objects.append(process_geary(mat))\n",
    "\n",
    "    m_speed = geary_objects[0]\n",
    "    m_vx = geary_objects[1]\n",
    "    m_vy = geary_objects[2]\n",
    "\n",
    "    # get matrices from geary objects\n",
    "    gearys_p_speed = np.reshape(m_speed.p_sim, mags.shape)\n",
    "    gearys_i_speed = np.reshape(m_speed.localG, mags.shape)\n",
    "    gearys_p_vx = np.reshape(m_vx.p_sim, mags.shape)\n",
    "    gearys_i_vx = np.reshape(m_vx.localG, mags.shape)\n",
    "    gearys_p_vy = np.reshape(m_vy.p_sim, mags.shape)\n",
    "    gearys_i_vy = np.reshape(m_vy.localG, mags.shape)\n",
    "\n",
    "    # ensure output directory exists\n",
    "    sac_dir = os.path.join(output_root, file_id_)\n",
    "    os.makedirs(sac_dir, exist_ok=True)\n",
    "\n",
    "    # save the geary object matrices\n",
    "    np.save(os.path.join(sac_dir, 'p_values_speed.npy'), gearys_p_speed)\n",
    "    np.save(os.path.join(sac_dir, 'i_values_speed.npy'), gearys_i_speed)\n",
    "    np.save(os.path.join(sac_dir, 'p_values_vx.npy'), gearys_p_vx)\n",
    "    np.save(os.path.join(sac_dir, 'i_values_vx.npy'), gearys_i_vx)\n",
    "    np.save(os.path.join(sac_dir, 'p_values_vy.npy'), gearys_p_vy)\n",
    "    np.save(os.path.join(sac_dir, 'i_values_vy.npy'), gearys_i_vy)\n",
    "\n",
    "    # save the registration parameters in the same folder\n",
    "    params = {\n",
    "        'neighborhood_size': NEIGHBORHOOD_SIZE,\n",
    "        'permutations': PERMUTATIONS,\n",
    "        'weights': 'constant = 1'\n",
    "    }\n",
    "    with open(os.path.join(sac_dir, 'params.json'), 'w') as json_f:\n",
    "        json.dump(params, json_f)\n",
    "        \n",
    "    print(file_id_, 'done', time.asctime())\n",
    "\n",
    "def save_geary_masks(file_id_):\n",
    "    sac_file_root = os.path.join(output_root, file_id_)\n",
    "    \n",
    "    # load each of the associated spatial autocorrelation processed files\n",
    "    # get matrices from geary objects\n",
    "    gearys_p_speed = np.load(os.path.join(sac_file_root, 'p_values_speed.npy'))\n",
    "    gearys_i_speed = np.load(os.path.join(sac_file_root, 'i_values_speed.npy'))\n",
    "    gearys_p_vx = np.load(os.path.join(sac_file_root, 'p_values_vx.npy'))\n",
    "    gearys_i_vx = np.load(os.path.join(sac_file_root, 'i_values_vx.npy'))\n",
    "    gearys_p_vy = np.load(os.path.join(sac_file_root, 'p_values_vy.npy'))\n",
    "    gearys_i_vy = np.load(os.path.join(sac_file_root, 'i_values_vy.npy'))\n",
    "    \n",
    "    # given the processed geary files, we can now create a geary mask based on i-value and p-value\n",
    "    # for the p-values, we are only interested in those regions where p<threshold\n",
    "    # so here we can convert the p-value matrices to boolean p-value masks\n",
    "    p_value_threshold = 0.05\n",
    "    gearys_p_speed_mask = gearys_p_speed < p_value_threshold\n",
    "    gearys_p_vx_mask = gearys_p_vx < p_value_threshold\n",
    "    gearys_p_vy_mask = gearys_p_vy < p_value_threshold\n",
    "\n",
    "    # for the i-values, we can look at areas of spatial clusters and spatially dispersed regions\n",
    "    # i value > 2 indicates that a feature has neighboring features with similarly high or low attribute values; this feature is part of a cluster.\n",
    "    # i value < 2 indicates that a feature has neighboring features with dissimilar values; this feature is an outlier.\n",
    "    # In either instance, the p-value for the feature must be small enough for the cluster or outlier to be considered statistically significant. \n",
    "    gearys_i_speed_mask = gearys_i_speed < 2\n",
    "    gearys_i_vx_mask = gearys_i_vx < 2\n",
    "    gearys_i_vy_mask = gearys_i_vy < 2\n",
    "    \n",
    "    # now we can combine a low p-value with positive i-value to get a mask for statistically significant correlated motion\n",
    "    geary_speed_mask = gearys_i_speed_mask & gearys_p_speed_mask\n",
    "    geary_vx_mask = gearys_i_vx_mask & gearys_p_vx_mask\n",
    "    geary_vy_mask = gearys_i_vy_mask & gearys_p_vy_mask\n",
    "    \n",
    "    # save the geary object matrices\n",
    "    sac_dir = os.path.join(output_root, file_id_)\n",
    "    np.save(os.path.join(sac_dir, 'speed_mask.npy'), geary_speed_mask)\n",
    "    np.save(os.path.join(sac_dir, 'vx_mask.npy'), geary_vx_mask)\n",
    "    np.save(os.path.join(sac_dir, 'vy_mask.npy'), geary_vy_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save spatial autocorrelation geary matrices (i-values and p-values)\n",
    "processed_root, register_root, output_root, file_ids = get_folder_information(experiment)\n",
    "print(experiment, '--------------------------')\n",
    "with Pool(NCPUS) as p:\n",
    "    list(tqdm(p.imap(process_files, file_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save geary masks\n",
    "processed_root, register_root, output_root, file_ids = get_folder_information(experiment)\n",
    "print(experiment, '--------------------------')\n",
    "with Pool(NCPUS) as p:\n",
    "    list(tqdm(p.imap(save_geary_masks, file_ids)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
