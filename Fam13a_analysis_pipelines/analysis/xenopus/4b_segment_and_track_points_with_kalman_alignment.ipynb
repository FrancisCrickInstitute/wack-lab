{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking of Xenopus Embryos using Kalman Filter\n",
    "This notebook uses segmentation masks generated with watershed to identify the centres of gravity of the embryos. These centres of mass are tracked across frames using a Kalman filter. The assignment of new measurements to existing tracks is based on the maximum likelihood of a given measurement belonging to a track, which is calculated from the covariance matrices extracted from the Kalman filter.\n",
    "\n",
    "### Next steps:\n",
    "- label paths on video for trouble shooting (optional)\n",
    "- show segmentation regions for trouble shooting (optional)\n",
    "- Apply kalman smoothing (optional)\n",
    "\n",
    "The user should set the paramers in the parameters section, where the id of the video to be considered can also be specified. Locations of the input and output data are specified in the cell data locations.\n",
    "\n",
    "Note: Running this notebook requires that initial markers are defined for the video to be processed. The notebook [4a_place_and_evaluate_markers](4a_place_and_evaluate_markers.ipynb) in this folder can be used to define those markers.\n",
    "Also note that running this notebook takes a long time (up to 4 hours on a 32-core node), so it is good to make sure enough time is available in the hpc ondemand session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "import cv2\n",
    "import os\n",
    "import copy\n",
    "import yaml\n",
    "\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "from fam13a import consts, utils, track, image, kalman\n",
    "from fam13a.paths import path, get_new_unique_label, get_path_by_label, check_paths_alive\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_ID = '5_L2_1' # Specify video to consider\n",
    "n_frames = -1 # Use -1 for considering all frames\n",
    "save_frequency = 500\n",
    "\n",
    "# Minimum length of path to be included in analysis\n",
    "MIN_LEN_PATH = 100\n",
    "\n",
    "# Number of frames we will propagate forwards without measurements until a path is considered dead.\n",
    "PROP_THD = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_ROOT = utils.here(True)\n",
    "VIDEO_ROOT = os.path.join(PROJ_ROOT, 'data', 'interim', 'xenopus')\n",
    "MARKERS_SEGMENTED_ROOT = os.path.join(PROJ_ROOT, 'data', 'processed', 'xenopus', 'segmented', 'markers')\n",
    "ANALYSIS_OUTPUT_DIR = os.path.join(PROJ_ROOT, 'data', 'processed', 'xenopus', 'statistics')\n",
    "VIDEO_OUTPUT_DIR = os.path.join(PROJ_ROOT, 'data', 'processed', 'xenopus', 'videos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set which video to track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MARKERS_SEGMENTED_ROOT, VIDEO_ID + \".yml\"), \"r\") as f:\n",
    "    points = yaml.load(f, Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestep between measurements/updates\n",
    "DT = 1\n",
    "\n",
    "# Assuming a random walk we can represent the state as x = [X, Y, X_dot, Y_dot]\n",
    "# Then the A matrix is the state model x_dot = A.x + G.w\n",
    "# where w is the state noise\n",
    "A = np.array([\n",
    "    [0,0,1,0],\n",
    "    [0,0,0,1],\n",
    "    [0,0,0,0],\n",
    "    [0,0,0,0]\n",
    "])\n",
    "PHI = sp.linalg.expm(A)\n",
    "\n",
    "# we set the SD \n",
    "SIG_WK = 0.03\n",
    "NOISE_ARR = np.array([SIG_WK, SIG_WK])\n",
    "\n",
    "SIG_VK = 100\n",
    "\n",
    "GAM = np.array([\n",
    "    [0.5*DT**2,0],\n",
    "    [0,0.5*DT**2],\n",
    "    [DT,0],\n",
    "    [0,DT]\n",
    "])\n",
    "\n",
    "H = np.array([[1,0,0,0],[0,1,0,0]])\n",
    "\n",
    "R = np.identity(2) * (SIG_VK**2)\n",
    "Q = np.array([\n",
    "    [SIG_WK**2, 0],\n",
    "    [0, SIG_WK**2],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up intitial conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kalman filter in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_frames = utils.frames_from_video(os.path.join(VIDEO_ROOT, f'{VIDEO_ID}.mp4'))\n",
    "if n_frames != -1:\n",
    "    raw_frames = raw_frames[:n_frames,...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Improvement suggestion for the future*: Instead of storing the videos, we should store the data that the video can be generated from. In the loop below, significant time is spent overwriting the video. In addition, it is not trivial to concatenate videos but it is relatively straight forward to concatenate data frames and lists, to then generate the videos afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track points across frames\n",
    "for frame_idx in tqdm(range(raw_frames.shape[0])):\n",
    "    \n",
    "    if frame_idx == 0:\n",
    "        new_markers = utils.points_to_markers(points, raw_frames[frame_idx, ...].shape)\n",
    "        new_markers = image.segment.process_markers(frame=cv2.cvtColor(raw_frames[frame_idx, ...],\n",
    "                                                                       cv2.COLOR_BGR2HSV),\n",
    "                                                    min_size=5000,\n",
    "                                                    max_size=18000,\n",
    "                                                    markers=new_markers)\n",
    "        paths = []\n",
    "        measure_uncert = [25,25]\n",
    "        position_estimates = []\n",
    "        for pt in utils.markers_to_pts(new_markers).values():\n",
    "            new_label = get_new_unique_label(paths)\n",
    "            new_path = path(new_label)\n",
    "            x_est, x_pred, P_pred, P_est = kalman.initialise_point(np.stack(pt, axis=0), *measure_uncert)\n",
    "            frame_idx = 0\n",
    "            new_path.add_predictions_to_track(x_pred, P_pred, frame_idx)\n",
    "            new_path.add_estimates_to_track(x_est, P_est, frame_idx)\n",
    "            new_path.z_meas.append((np.stack(pt, axis=0), frame_idx))\n",
    "            paths.append(new_path)\n",
    "            position_estimates.append(x_est[0:2])\n",
    "        prev_pts = np.squeeze(position_estimates)\n",
    "    \n",
    "    curr_pts = utils.markers_to_pts(new_markers)\n",
    "    new_markers = image.segment.process_markers(frame=cv2.cvtColor(raw_frames[frame_idx, ...],\n",
    "                                                                   cv2.COLOR_BGR2HSV),\n",
    "                                                min_size=5000,\n",
    "                                                max_size=18000,\n",
    "                                                markers=new_markers)\n",
    "    \n",
    "    frame_idx +=1 # Increment as frame_idx starts counting from 0 but we'v already processed the 0th frame above.\n",
    "    tracked_pts = []\n",
    "    \n",
    "    # Get measurements from the current frame\n",
    "    curr_pts = list(curr_pts.values())\n",
    "    if curr_pts:\n",
    "        curr_pts = np.stack(curr_pts, axis=0)\n",
    "    else:\n",
    "        # Why are we setting all points to zero if there are no points?\n",
    "        curr_pts = np.empty((0, *prev_pts.shape[1:]))\n",
    "        \n",
    "    # Propagate paths from the previous estimate\n",
    "    paths = kalman.propagate_paths(paths, frame_idx, PHI, GAM, Q)\n",
    "    \n",
    "    # Mark paths that have not been propagated for more than PROP_THD as dead\n",
    "    # TODO: consider errors on the state estimate as a threshold.\n",
    "    paths = check_paths_alive(paths, PROP_THD)\n",
    "\n",
    "    curr_pts_idxs, path_labels = kalman.align_measurements_to_paths(paths, curr_pts, H, R)\n",
    "    paths, new_pts = kalman.assign_measurements_to_paths(paths, curr_pts, curr_pts_idxs, path_labels, frame_idx)\n",
    "\n",
    "    # Add new paths for new points:\n",
    "    # Get any new measurements (unassigned in the munkres algorithm)\n",
    "    paths = kalman.append_new_paths(new_pts, measure_uncert, frame_idx, paths)\n",
    "    \n",
    "    # Update estimates for paths with measurements\n",
    "    paths = kalman.update_estimates_on_paths_with_measurements(paths, H, R, PHI)\n",
    "    \n",
    "    if frame_idx % save_frequency == 0:\n",
    "        utils.save_video(copy.deepcopy(raw_frames[:frame_idx, ...]), copy.deepcopy(paths), os.path.join(VIDEO_OUTPUT_DIR, VIDEO_ID))\n",
    "        print(\"Saved video\")\n",
    "utils.save_video(copy.deepcopy(raw_frames[:frame_idx, ...]), copy.deepcopy(paths), os.path.join(VIDEO_OUTPUT_DIR, VIDEO_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a path and plot the estimation errors\n",
    "idx_path = 5\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(2, 1), aspect=False, share_all=True)\n",
    "\n",
    "for idx, ax in enumerate(grid):\n",
    "    frames = [frame_id for P_est, frame_id in paths[idx_path].P_est]\n",
    "    x_est = [x_est[idx] for x_est, frame_id in paths[idx_path].x_est]\n",
    "    z_meas = [z_meas[idx] for z_meas, frame_id in paths[idx_path].z_meas]\n",
    "    z_diff = [x - z for x, z in zip(x_est, z_meas)]\n",
    "    ax.plot(frames, z_diff)\n",
    "    ax.plot(frames, np.sqrt([P_est[idx,idx] for P_est, frame_id in paths[idx_path].P_est]), 'r:')\n",
    "    ax.plot(frames, -np.sqrt([P_est[idx,idx] for P_est, frame_id in paths[idx_path].P_est]), 'r:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out all paths that have a length which is shorter than the defined threshold MIN_LEN_PATH\n",
    "filt_paths = [p for p in paths if len(p.x_est) > MIN_LEN_PATH]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_velocity(path):\n",
    "    velocity = [float(np.sqrt(pow(x[0][2], 2) + pow(x[0][3], 2))[0]) for x in path.x_est]\n",
    "    return velocity\n",
    "\n",
    "vel_per_path = [get_velocity(x) for x in filt_paths]\n",
    "avg_vel_per_path = [float(np.mean(x)) for x in vel_per_path]\n",
    "all_vel = [x for y in vel_per_path for x in y]\n",
    "avg_vel_video = float(np.mean(all_vel))\n",
    "std_vel_per_path = [float(np.std(x)) for x in vel_per_path]\n",
    "std_vel_video = float(np.std(all_vel))\n",
    "\n",
    "result = dict({\n",
    "    \"vel_per_path\": vel_per_path,\n",
    "    \"avgvel_per_path\": avg_vel_per_path,\n",
    "    \"all_vel\": all_vel,\n",
    "    \"avg_vel_video\": avg_vel_video,\n",
    "    \"std_vel_per_path\": std_vel_per_path,\n",
    "    \"std_vel_video\": std_vel_video\n",
    "})\n",
    "\n",
    "with open(os.path.join(ANALYSIS_OUTPUT_DIR, VIDEO_ID + \".yml\"), \"w\") as f:\n",
    "    yaml.dump([result], f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_fam13a-dev)",
   "language": "python",
   "name": "conda_fam13a-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
